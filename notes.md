### Things I am going through
- https://towardsdatascience.com/mnist-handwritten-digits-classification-from-scratch-using-python-numpy-b08e401c4dab

- https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/
- https://www.youtube.com/watch?v=w8yWXqWQYmU
- https://avi.alkalay.net/2018/07/fedora-jupyter-notebook.html
- https://en.wikipedia.org/wiki/MNIST_database
- tqdm (progress bar)
- https://www.kaggle.com/datasets/hojjatk/mnist-dataset
- https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook

### Notes
- Gradient descent is basic machine learning algo
- y = w(x) OR y = w(x) - b
- goal -> w(x) - b = 0
- square result from cost function to get more amplified result
- input is usually a feature vector
	1. Any value for w
	2. Give to w to cost function to get prediction precision (close to 0, the more precise and accurate)
	3. w - derivative of cost function (limit as h->0)
	4. Apply learning rate
	5. Iterate many times

### Ideas
- Python neural network class to use for other things
- Machine learning library in C
- Crypto Predicition
- Something like ChatGPT that sites it's sources will over take Google as a search engine
