## Stanford lecture notes

### Lecture 3
- Batch gradient descent:
    - Is limited as it has to go through whole dataset for each iteration of training
- Stochastic gradient descent:
